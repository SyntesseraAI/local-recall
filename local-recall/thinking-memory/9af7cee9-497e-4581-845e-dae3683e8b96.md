---
id: 9af7cee9-497e-4581-845e-dae3683e8b96
subject: >-
  The user is concerned that if they run multiple Claude instances (each with
  their own MCP server),...
applies_to: global
occurred_at: '2025-12-03T11:30:35.467Z'
content_hash: 3c65ef533184d540
---
## Thought

The user is concerned that if they run multiple Claude instances (each with their own MCP server), each would load its own ONNX runtime instance, and that might cause mutex issues between them.

Actually, let me think about this more carefully:

1. If each Claude instance runs in a separate process with its own MCP server
2. Each MCP server loads ONNX/fastembed independently
3. Multiple ONNX instances in different processes...

Wait, the mutex error is happening when multiple processes try to load ONNX concurrently within the same system. The error is "mutex lock failed: Invalid argument" which is a low-level libc++ error. This could happen when:
- Multiple processes try to initialize ONNX at the same time
- Or there's some shared resource (like GPU, or shared memory) that conflicts

Let me think... the mutex error is from ONNX runtime's internal threading. When multiple Node.js processes start simultaneously and all try to load the ONNX model, they might conflict at the system level.

So yes, even with separate MCP servers per Claude instance, if you start multiple Claude instances at once, you could still get mutex errors during the initialization phase.

The user's concern is valid. What they really need is:
1. A SINGLE embedding service that all Claude instances share
2. OR an embedding solution that doesn't have mutex issues at all

For option 1 - a single shared embedding daemon:
- Could be started once (like Ollama)
- All Claude instances connect to it
- But user said they don't want this because it means they can't run multiple instances

Actually wait, re-reading: "it means I can't run multiple instances of claude at once" - I think the user is worried that a daemon approach would somehow prevent multiple Claude instances. But a properly designed shared daemon (like Ollama) DOES support multiple clients. So maybe there's a misunderstanding.

Let me clarify with the user: a shared embedding daemon (like Ollama, or a custom one) can support multiple Claude instances simultaneously. The mutex issue is specifically about multiple processes loading the ONNX model, not about multiple clients using an already-loaded model.


I see two potential alternative approaches to resolve the mutex problem. I could explore pure WebAssembly solutions that avoid native bindings entirely, eliminating threading complications. Alternatively, leveraging a remote API service like OpenAI's embeddings could sidestep local runtime initialization challenges. These options provide clean workarounds to the concurrent loading issue.

## Output

You're right. Multiple Claude instances = multiple ONNX loads = same mutex risk during startup.

The core issue: **ONNX runtime has system-level mutex conflicts when multiple processes initialize concurrently**, regardless of whether they're hooks or MCP servers.

**Options:**

1. **Single shared embedding daemon** (like Ollama) - Actually supports multiple Claude instances simultaneously. The mutex issue is about *loading* ONNX, not *using* an already-loaded model. One daemon, many clients = no conflicts.

2. **Remote embedding API** (OpenAI, Cohere, Voyage) - No local model loading at all. Requires API key + internet.

3. **Pure WASM embeddings** (no native bindings) - Transformers.js can run in WASM-only mode. Slower but no native mutex issues. Need to verify this works.

4. **Serialize all embedding loads** - Single global lock file that ALL processes (hooks + MCP servers) wait on. Clunky but might work.

For option 1: A shared daemon doesn't prevent multiple Claudes - it enables them. Ollama handles dozens of concurrent requests fine. The question is whether you want a dependency on Ollama or want something self-contained in npm.

Which tradeoff works best for your use case?
