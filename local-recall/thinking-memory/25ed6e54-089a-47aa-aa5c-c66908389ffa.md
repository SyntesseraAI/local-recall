---
id: 25ed6e54-089a-47aa-aa5c-c66908389ffa
subject: >-
  Let me analyze this transcript to extract valuable memories for future AI
  assistants working on the...
applies_to: global
occurred_at: '2025-12-21T19:33:06.564Z'
content_hash: 18c299e07bb97b89
---
## Thought

Let me analyze this transcript to extract valuable memories for future AI assistants working on the local-recall codebase.

The user is asking for documentation updates and then moves into a discussion about:
1. Making the MCP server run as a stdio process (it already does)
2. Multiple local-recall instances running and potentially cross-communicating
3. The issue with shared Orama indexes when multiple processes write to them
4. A proposal for a new architecture where:
   - Embeddings are stored as sidecar files next to memories
   - A "no-embeddings" mode for cloud agents that can't call Ollama
   - Memories created without embeddings can be embedded later by local processes
   - Standard mode tries to load from disk embeddings first, falls back to Ollama

Key insights from this session:
1. The current architecture has a concurrency problem when multiple instances try to update the Orama indexes
2. The solution being discussed is to store embeddings as separate files so they can be version controlled
3. There's a need for a "no-embeddings" mode for cloud agents
4. The vector store needs to be able to load from disk embeddings and fall back to Ollama generation

Let me extract the important memories:

1. **Orama Index Concurrency Problem** - Multiple processes writing to the same index JSON file causes lost updates. This applies to both episodic and thinking indexes.

2. **Cloud Agent Memory Creation** - Cloud agents need to be able to create memories without access to Ollama/embeddings. They should be able to create memory files that local processes can later embed.

3. **Embedding Storage Strategy** - The proposal is to store embeddings as sidecar `.embedding.json` files next to memory files so they can be version controlled and shared.

4. **Two-Mode System** - Need a "no-embeddings" mode for cloud agents that loads from disk embeddings only, and a standard mode that tries disk embeddings first, then falls back to Ollama.
 
I'll focus on project isolation to prevent cross-contamination between different development environments. Each project will maintain its own dedicated local-recall directory, ensuring that memories and indexes remain strictly separated. This approach prevents unintended interactions between different coding sessions or project contexts, maintaining clean and isolated memory management for each unique development workspace. I'll generate embeddings locally when possible, falling back to Ollama for vector creation if needed. The process ensures cloud agents can create memory files, while local processes handle embedding generation efficiently. This approach allows seamless memory contribution across different computing environments, with embedding vectors stored conveniently next to their corresponding memory files.

The embedding file format includes metadata like model type, dimensions, creation timestamp, and the actual vector data. By tracking these files alongside memory markdown files, we enable version control, easy index rebuilding, and minimize potential write conflicts during memory management.

The loading strategy prioritizes disk-based embeddings, with configurable modes to handle scenarios where embedding generation might be unnecessary or computationally expensive. This flexible approach supports distributed memory creation while maintaining a consistent, efficient vector storage mechanism.

Project isolation ensures that each codebase maintains its own local recall directory, preventing cross-contamination between different instances. By segregating memory files and Orama indexes, the system guarantees that concurrent processes within the same project can safely interact without risking data interference or unintended overwrites.

The proposed architecture supports two distinct operational modes: a standard local mode with full embedding capabilities and a cloud-optimized mode designed for environments with limited embedding generation resources. This flexible approach allows seamless adaptation across different deployment scenarios.

## Output

```json
{
  "memories": [
    {
      "subject": "Orama index concurrency issue with multiple processes writing simultaneously",
      "keywords": ["orama", "concurrency", "index-corruption", "multi-process", "lost-updates", "race-condition", "json-index"],
      "applies_to": "global",
      "content": "## Problem\n\nWhen multiple local-recall instances (different Claude Code sessions) work on the same project, they all write to the same Orama index files (`orama-episodic-index.json`, `orama-thinking-index.json`) without coordination.\n\n## Root Cause\n\nEach Claude Code instance spawns its own MCP server process. Both the hooks and MCP daemon independently:\n- Load the Orama index from disk\n- Add/remove memories\n- Write the entire index back to disk\n\nThis creates a race condition where later writes overwrite previous changes, causing lost updates.\n\n## When This Occurs\n\n- Multiple Claude Code sessions running on the same project simultaneously\n- Both instances adding memories or updating the vector store\n- Affects both episodic and thinking indexes\n\n## When This Is NOT a Problem\n\n- Each project has its own `LOCAL_RECALL_DIR` directory (completely isolated)\n- Different projects never interfere with each other\n- Single instance per project works fine"
    },
    {
      "subject": "Cloud agents need no-embeddings mode to create memories without Ollama",
      "keywords": ["cloud-agent", "no-embeddings-mode", "mcp-server", "memory-creation", "offline", "server-side"],
      "applies_to": "global",
      "content": "## Requirement\n\nCloud agents running on servers typically:\n- Don't have Ollama installed or accessible\n- Need to create and save memories via MCP tools\n- Can't generate embedding vectors\n- Need to push memory files to source control for local processes to embed\n\n## Design Pattern\n\n### Cloud Agent Mode (no-embeddings)\n- Create memory markdown files directly via MCP tools\n- Skip embedding generation entirely\n- No Ollama calls\n- Memory files are git-tracked and pushed to repo\n\n### Local Mode (standard)\n- Try loading embeddings from disk first (`.embedding.json` sidecar)\n- If embedding missing, call Ollama to generate it\n- Persist generated embedding to disk\n\n## Configuration\n\n- `LOCAL_RECALL_NO_EMBEDDINGS=true` - Cloud agent mode (skip Ollama)\n- Default/unset - Standard mode (Ollama fallback for missing embeddings)"
    },
    {
      "subject": "Store embeddings as sidecar JSON files next to memory files",
      "keywords": ["embedding-storage", "sidecar-file", ".embedding.json", "disk-persistence", "version-control", "vector-vector"],
      "applies_to": "global",
      "content": "## Architecture\n\nStore embedding vectors as separate files alongside memory markdown:\n\n```\nepisodic-memory/\n├── a1b2c3d4.md                 ← Memory content (git tracked)\n├── a1b2c3d4.embedding.json     ← Embedding vector (git tracked)\n├── e5f6g7h8.md                 ← Created by cloud agent\n└── (no e5f6g7h8.embedding)     ← Missing until local process embeds\n```\n\n## Embedding File Format\n\n```json\n{\n  \"model\": \"nomic-embed-text\",\n  \"model_version\": \"1.0\",\n  \"dimensions\": 768,\n  \"created_at\": \"2025-01-15T10:30:00Z\",\n  \"vector\": [0.123, 0.456, ..., 0.789]\n}\n```\n\n## Benefits\n\n1. **Reduces Orama index conflicts** - Index can be regenerated from disk embeddings\n2. **Enables cloud agent participation** - Cloud can create memories, local embeds them\n3. **Version-controllable** - Embeddings tracked in git alongside memories\n4. **Lazy embedding** - Local process can batch/schedule embedding generation\n5. **Reproducible** - Same embedding for same memory (deterministic model)"
    },
    {
      "subject": "Vector store two-phase loading: disk embeddings then Ollama fallback",
      "keywords": ["vector-store", "lazy-loading", "embedding-loading", "fallback", "disk-cache", "ollama-fallback"],
      "applies_to": "file:src/core/vector-store.ts",
      "content": "## Loading Strategy\n\nWhen initializing or syncing memories with the vector store:\n\n### Phase 1: Load from Disk\n1. Check if `<memory-id>.embedding.json` exists\n2. If exists, parse and use the stored vector\n3. Skip to Phase 2 for next memory\n\n### Phase 2: Generate if Missing\n1. If no disk embedding found:\n   - Check if `LOCAL_RECALL_NO_EMBEDDINGS=true`\n   - If true: log and skip (don't call Ollama)\n   - If false: call Ollama to generate embedding\n2. Persist generated embedding to `<memory-id>.embedding.json`\n3. Continue to next memory\n\n## Implementation Pattern\n\n```typescript\nfor (const memory of memories) {\n  // Try loading from disk first\n  const diskEmbedding = await loadEmbeddingFile(memory.id);\n  if (diskEmbedding) {\n    addToIndex(memory, diskEmbedding);\n    continue;\n  }\n  \n  // Fallback to Ollama generation\n  if (noEmbeddingsMode) continue; // Skip\n  const generated = await ollamaEmbed(memory.content);\n  await persistEmbeddingFile(memory.id, generated);\n  addToIndex(memory, generated);\n}\n```\n\n## Advantages\n\n- Same codebase works in both cloud (no-embeddings) and local (with Ollama) modes\n- No code path branching needed\n- Graceful degradation when Ollama unavailable"
    },
    {
      "subject": "Project isolation via LOCAL_RECALL_DIR prevents cross-contamination",
      "keywords": ["project-isolation", "local-recall-dir", "separate-codebase", "environment-variable", "multi-project"],
      "applies_to": "global",
      "content": "## Key Finding\n\nEach project configures its own memory directory via `LOCAL_RECALL_DIR` environment variable. This provides complete isolation:\n\n```\n~/project-alpha/\n└── local-recall/           ← Only project-alpha instances write here\n    ├── episodic-memory/\n    ├── thinking-memory/\n    ├── orama-episodic-index.json\n    └── orama-thinking-index.json\n\n~/project-beta/\n└── local-recall/           ← Separate storage, never interferes\n    ├── episodic-memory/\n    ├── thinking-memory/\n    ├── orama-episodic-index.json\n    └── orama-thinking-index.json\n```\n\n## Important Implication\n\nConcurrency/write-conflict issues with Orama indexes ONLY occur when:\n- Multiple Claude Code instances run on the **same** project\n- Both try to update the same `orama-*.json` files simultaneously\n\nDifferent projects are completely isolated and never interfere.\n\n## Recommendation\n\nDocument that:\n- Single instance per project: no concurrency worries\n- Multiple instances same project: risk of index conflicts\n- Solution: Use embedding sidecar files to allow index regeneration"
    },
    {
      "subject": "Documentation comprehensively updated for new user onboarding",
      "keywords": ["documentation", "readme", "hooks", "configuration", "mcp-server", "thinking-memory", "onboarding"],
      "applies_to": "global",
      "content": "## Documentation Updates\n\nAll major documentation files were rewritten to reflect current architecture:\n\n### Files Updated\n1. **README.md** - Complete rewrite with current features (Ollama, Thinking Memories, correct MCP tools)\n2. **docs/hooks.md** - Updated from old SQLite+fastembed to Orama+Ollama, unified UserPromptSubmit hook\n3. **docs/configuration.md** - New config options, corrected defaults (LOG_LEVEL→error, port fixed)\n4. **docs/mcp-server.md** - Removed SQLite references, updated tool list\n5. **docs/thinking-memory.md** - Major expansion with examples and reset instructions\n6. **CLAUDE.md** - Fixed Stop hook configuration, added missing LOG_LEVEL env var\n\n### Key Fixes\n- Corrected MCP tool names: `episodic_create`, `episodic_get`, `episodic_search`, `thinking_get`, `thinking_search`\n- Removed obsolete references to SQLite, fastembed\n- Added Ollama prerequisite instructions\n- Fixed configuration option defaults\n- Added thinking memory examples\n\n### Commit\nCommit `ca8cd80`: \"docs: Comprehensive documentation update for new users\" (+724/-244 lines)"
    }
  ]
}
```
