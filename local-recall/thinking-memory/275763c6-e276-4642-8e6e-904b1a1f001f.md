---
id: 275763c6-e276-4642-8e6e-904b1a1f001f
subject: >-
  The user wants to enable a mode where: 1. **Cloud/server agents** can create
  memories via MCP...
applies_to: global
occurred_at: '2025-12-15T14:57:56.267Z'
content_hash: b29543e871d266d1
---
## Thought

The user wants to enable a mode where:

1. **Cloud/server agents** can create memories via MCP without needing Ollama (no embeddings created)
2. **Embeddings stored separately** - as sidecar files next to each memory
3. **Two modes**:
   - `no-embeddings` mode: Can read existing embeddings from disk, create memories without embeddings
   - `standard` mode: Load from disk if exists, else generate via Ollama

Let me think through this architecture deeply:

## Current State
- Memory files: `episodic-memory/{uuid}.md`
- Embeddings: Stored inside Orama index (`orama-episodic-index.json`)
- Index is gitignored, regenerated via `sync()`

## Proposed Architecture

### Embedding Sidecar Files
```
episodic-memory/
├── abc123.md           ← Memory content (git tracked)
├── abc123.embedding    ← Embedding vector (git tracked)
├── def456.md
├── def456.embedding
└── ...
```

### File Format for `.embedding`
Could be:
- JSON: `{"vector": [0.1, 0.2, ...], "model": "nomic-embed-text", "dim": 768}`
- Binary: More compact but harder to debug
- Base64 JSON: Compromise

### Modes

**no-embeddings mode** (cloud agents):
- Can CREATE memories (writes `.md` file, no `.embedding` file)
- Can READ memories
- Can SEARCH if embeddings exist on disk (loads `.embedding` files into Orama)
- Cannot generate new embeddings

**standard mode** (local with Ollama):
- Can CREATE memories (writes `.md` + generates `.embedding`)
- Can READ memories
- Can SEARCH (loads existing `.embedding` or generates missing ones)
- Generates embeddings for any memory missing its `.embedding` file
 
I'll walk through a synchronization scenario that highlights the cross-mode interaction. When a cloud agent creates a memory without an embedding, the local system can automatically generate and fill in the missing embedding during synchronization. This approach ensures seamless integration between different deployment environments, allowing cloud agents to create memories lightweight while local systems enrich the memory with semantic search capabilities.

The process involves:
- Cloud agent creates a bare memory file
- Local system detects the unembedded memory
- Automatic embedding generation fills the gap
- Semantic search becomes possible after sync

The key innovation is the flexible configuration allowing different embedding strategies across distributed systems, with local Ollama instances acting as intelligent embedding generators.

Embedding file management introduces nuanced logic:
- Prioritize existing embedding files
- Conditionally generate based on configuration mode
- Enable partial searchability even without full vector representations

Vector store modifications support granular memory indexing:
- Selective embedding generation
- Fallback retrieval mechanisms
- Preserving memory accessibility across varied computational environments

Git tracking ensures portability and synchronization of memory artifacts, maintaining a consistent knowledge representation strategy.

Cloud and local deployment scenarios benefit from flexible embedding generation, allowing incremental knowledge capture without strict infrastructure dependencies.

Potential implementation challenges include model consistency and file management overhead, balanced against enhanced cross-machine knowledge transfer capabilities.

The proposed file-based approach prioritizes portability and collaborative knowledge generation, with individual embedding files preventing synchronization conflicts between multiple agents.

## Output

Let me think through this architecture deeply.

## Proposed Architecture: Sidecar Embedding Files

```
episodic-memory/
├── abc123.md              ← Memory content (git tracked)
├── abc123.embedding.json  ← Embedding vector (git tracked)
├── def456.md              ← Created by cloud agent
├── (no def456.embedding)  ← Missing until local catches up
└── ...
```

### Embedding File Format
```json
{
  "model": "nomic-embed-text",
  "dimensions": 768,
  "created_at": "2025-01-15T10:30:00Z",
  "vector": [0.123, -0.456, ...]
}
```

## Two Modes

| Mode | Create Memory | Generate Embedding | Load Existing Embedding | Search |
|------|---------------|-------------------|------------------------|--------|
| `no-embeddings` | ✓ writes `.md` | ✗ | ✓ from `.embedding.json` | Only memories with embeddings |
| `standard` | ✓ writes `.md` | ✓ via Ollama | ✓ from `.embedding.json` | All memories (generates missing) |

## The Catch-Up Flow

```
Timeline:
═══════════════════════════════════════════════════════════════════════════

Cloud Agent (no-embeddings mode):
  1. Creates memory → writes abc123.md
  2. No Ollama → no embedding generated
  3. git push abc123.md
  
Local Dev (standard mode):
  4. git pull → gets abc123.md (no embedding)
  5. sync() runs:
     • Sees abc123.md exists
     • No abc123.embedding.json found
     • Generates embedding via Ollama
     • Writes abc123.embedding.json
  6. git push abc123.embedding.json

Cloud Agent (no-embeddings mode):
  7. git pull → gets abc123.embedding.json
  8. sync() runs:
     • Loads embedding from disk
     • Adds to Orama index
  9. Can now search for abc123! ✓

═══════════════════════════════════════════════════════════════════════════
```

**Yes! This enables catch-up.** Local machines become "embedding generators" that commit `.embedding.json` files for cloud agents to use.

## Implementation Changes

### 1. New Config Option
```typescript
// types.ts
embeddingMode: z.enum(['standard', 'no-embeddings']).default('standard'),
```

### 2. VectorStore.add() Logic
```typescript
async add(memory: Memory): Promise<void> {
  const embeddingPath = this.getEmbeddingPath(memory.id);
  let embedding: number[];
  
  // Try loading from disk first
  const diskEmbedding = await this.loadEmbeddingFromDisk(embeddingPath);
  
  if (diskEmbedding) {
    embedding = diskEmbedding.vector;
  } else if (this.mode === 'standard') {
    // Generate and save
    embedding = await this.embeddingService.embed(memory);
    await this.saveEmbeddingToDisk(embeddingPath, embedding);
  } else {
    // no-embeddings mode: skip, memory not searchable yet
    logger.info(`Memory ${memory.id} created without embedding (no-embeddings mode)`);
    return;
  }
  
  // Add to Orama index
  await insert(this.db, { ...memory, embedding });
}
```

### 3. Git Tracking
```gitignore
# Keep these TRACKED (remove from .gitignore):
# episodic-memory/*.embedding.json  ← NEW: embeddings are portable

# Keep these IGNORED:
orama-episodic-index.json           ← Cache, regenerated from files
```

## Benefits

1. **Cloud agents work without Ollama** - create memories, commit them
2. **Local catches up** - generates missing embeddings, commits them back
3. **No merge conflicts** - each embedding is its own file
4. **Portable embeddings** - same embedding works on any machine
5. **Incremental** - only generate missing embeddings

## Potential Concerns

| Concern | Mitigation |
|---------|------------|
| Model drift (different Ollama versions) | Store model name in file, warn on mismatch |
| File count doubles | Acceptable tradeoff (~3KB per embedding) |
| Search returns partial results | Document that no-embeddings mode has limited search |

Want me to implement this? I'll start with:
1. Add `embeddingMode` config option
2. Create embedding sidecar file read/write logic
3. Update VectorStore to use disk embeddings
4. Update `.gitignore` to track embedding files
