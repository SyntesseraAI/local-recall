---
id: 9077cdbe-404d-4c25-b20d-f95b59308a6f
subject: >-
  Looking at this transcript, I need to extract memories about what was learned
  during the migration...
applies_to: global
occurred_at: '2025-12-03T11:45:54.189Z'
content_hash: 90e4bd83f1f53a3c
---
## Thought

Looking at this transcript, I need to extract memories about what was learned during the migration from fastembed to Ollama for embeddings. Let me analyze:

1. **What was learned?**
   - The project was migrated from fastembed (a Python-based embedding model using BGE-small-en-v1.5 with 384 dimensions) to Ollama (using nomic-embed-text model with 768 dimensions)
   - Ollama is a lighter, faster solution for local embeddings
   - The nomic-embed-text model is described as "best & fastest model possible while keeping it very small"
   - The migration involved updating EmbeddingService, vector store initialization, package.json, configuration, and documentation
   - The local_cache directory was used by fastembed and should be removed when migrating
   - Dependencies removed: fastembed, proper-lockfile

2. **What is now known about the codebase?**
   - Embeddings are generated using Ollama with the `nomic-embed-text` model (768 dimensions)
   - Ollama server runs on http://localhost:11434 by default
   - Environment variables for Ollama: OLLAMA_BASE_URL and OLLAMA_EMBED_MODEL
   - The vector store uses Orama (pure JavaScript) for vector storage and search
   - The architecture follows a hook-daemon pattern where hooks communicate via HTTP to avoid sqlite-vec mutex issues

3. **What problems were solved?**
   - Removed redundant fastembed dependency which was slower and larger
   - Cleaned up local_cache references from gitignore and eslint config
   - Updated documentation to reflect the new architecture (CLAUDE.md and docs/architecture.md)
   - The migration maintains the same interface but uses Ollama instead of fastembed

4. **Specific to this codebase?**
   - The project uses both episodic and thinking memories
   - Configuration includes episodicEnabled, episodicMaxTokens, episodicMinSimilarity, thinkingEnabled, thinkingMaxTokens, thinkingMinSimilarity
   - The embedding dimension changed from 384 to 768, necessitating index rebuilding
   - Old indexes must be deleted: removing local-recall/orama-episodic-index.json and local-recall/orama-thinking-index.json
   - HTTP-based communication prevents potential mutex lock failures

I'll focus on the key migration insights: switching to a more efficient embedding model required deleting existing indexes and updating configuration to support the new 768-dimensional Ollama embeddings. The migration streamlines the embedding process while maintaining core functionality.

The hook-daemon approach provides robust communication, mitigating potential database synchronization challenges with sqlite-vec. This architectural choice ensures more stable index management during model transitions.

## Output

```json
{
  "memories": [
    {
      "subject": "Migrated from fastembed to Ollama for embeddings",
      "keywords": ["ollama", "embeddings", "migration", "nomic-embed-text", "architecture"],
      "applies_to": "global",
      "content": "# Embedding Service Migration to Ollama\n\nThe project was successfully migrated from fastembed (BGE-small-en-v1.5, 384 dimensions) to Ollama (nomic-embed-text, 768 dimensions). This provides better performance and smaller model size while maintaining higher dimensional embeddings (768 vs 384).\n\n## Key Changes:\n- **Model**: nomic-embed-text (768 dimensions)\n- **Service**: Ollama running on http://localhost:11434 (default)\n- **Configuration**: OLLAMA_BASE_URL and OLLAMA_EMBED_MODEL environment variables\n- **Dependencies Removed**: fastembed, proper-lockfile\n- **Storage**: Vector store uses Orama (pure JavaScript) for both episodic and thinking memories\n\n## Migration Notes:\n- When changing embedding dimensions, old vector indexes must be deleted and rebuilt:\n  - `rm local-recall/orama-episodic-index.json`\n  - `rm local-recall/orama-thinking-index.json`\n- Ollama must be installed and running: `ollama serve`\n- Pull the model: `ollama pull nomic-embed-text`\n- No code changes needed for downstream consumers (same EmbeddingService interface)"
    },
    {
      "subject": "Removed local_cache directory references from build system",
      "keywords": ["local_cache", "fastembed", "gitignore", "eslint", "cleanup"],
      "applies_to": "global",
      "content": "# Cleanup of local_cache References\n\nRemoving fastembed eliminated the need for the local_cache directory which was used to store downloaded embedding model files.\n\n## Files Updated:\n- **eslint.config.js**: Removed 'local_cache/**' from ignores array\n- **.gitignore**: Removed local_cache references (was ~274MB model storage)\n\n## Environment Setup:\nUnlike fastembed which cached models locally, Ollama manages model storage independently through its own caching mechanism."
    },
    {
      "subject": "Hook-daemon architecture uses HTTP to prevent sqlite-vec mutex errors",
      "keywords": ["hook-daemon", "http", "sqlite-vec", "architecture", "localhost:7847", "concurrency"],
      "applies_to": "global",
      "content": "# Hook-Daemon Architecture Pattern\n\nThe project uses a hook-daemon architecture where Claude Code hooks communicate with an MCP daemon via HTTP on localhost:7847. This design prevents \"mutex lock failed: Invalid argument\" errors that occur when multiple processes try to load the sqlite-vec native extension concurrently.\n\n## Architecture:\n```\nHook Process (thin client) ←HTTP→ MCP Daemon (owns sqlite-vec)\n  • DaemonClient                • HTTP Server\n  • No sqlite-vec               • Vector Store\n  • Falls back to files          • Search Engines\n```\n\n## HTTP Endpoints (MCP Daemon):\n- `POST /search/episodic` - Search episodic memories\n- `POST /search/thinking` - Search thinking memories  \n- `POST /memories/recent` - Get recent memories\n- `GET /health` - Health check\n\n## Why This Matters:\n- Hooks run in separate processes from the daemon\n- Only the daemon process loads sqlite-vec (no concurrent access)\n- Hooks have fallback behavior when daemon is unavailable"
    },
    {
      "subject": "Documentation updated for Ollama embedding architecture",
      "keywords": ["documentation", "ollama", "CLAUDE.md", "architecture.md", "embedding model"],
      "applies_to": "global",
      "content": "# Documentation Updates\n\nCLAUDE.md and docs/architecture.md were updated to reflect the Ollama-based embedding architecture.\n\n## Changes Made:\n- Embedding service section updated to reference Ollama instead of fastembed\n- Vector store description updated to reflect Orama (pure JavaScript) storage\n- Removed references to local_cache and model downloading\n- Added Ollama installation and model pulling instructions\n- Updated troubleshooting section with Ollama-specific guidance\n- Explained embedding dimension migration from 384 to 768\n\n## Current State:\n- All documentation reflects Ollama as the embedding service\n- Configuration options documented for OLLAMA_BASE_URL and OLLAMA_EMBED_MODEL\n- Clear migration path documented for users upgrading from fastembed"
    },
    {
      "subject": "Thinking and episodic memory configuration with similar options",
      "keywords": ["thinking-memory", "episodic-memory", "configuration", "similarity", "tokens"],
      "applies_to": "global",
      "content": "# Memory Configuration Options\n\nBoth episodic and thinking memories share similar configuration patterns:\n\n## Episodic Memory:\n- `episodicEnabled` - Enable/disable episodic memory retrieval (default: true)\n- `episodicMaxTokens` - Maximum tokens to inject per prompt (default: 1000)\n- `episodicMinSimilarity` - Minimum similarity threshold 0.0-1.0 (default: 0.8)\n\n## Thinking Memory:\n- `thinkingEnabled` - Enable/disable thinking memory retrieval (default: true)\n- `thinkingMaxTokens` - Maximum tokens to inject per prompt (default: 1000)\n- `thinkingMinSimilarity` - Minimum similarity threshold 0.0-1.0 (default: 0.8)\n\n## Scoring:\n- Uses cosine distance similarity (0.0 = no match, 1.0 = identical)\n- Scores rounded to 2 decimal places\n- Recency is used as tie-breaker when scores are equal (more recent memories ranked first)\n\nBoth memory types work with the same underlying Orama vector store using Ollama embeddings."
    }
  ]
}
```
