---
id: 83f629b2-ad85-4fcf-9c98-c3cf44043efb
subject: >-
  Ollama looks like a good option - it runs as a separate server process and
  handles concurrency via...
applies_to: global
occurred_at: '2025-12-03T11:23:55.773Z'
content_hash: fd62384cd8d139d3
---
## Thought

Ollama looks like a good option - it runs as a separate server process and handles concurrency via HTTP API. Multiple hook processes can all call the same Ollama server without mutex issues.

Let me summarize the options for the user:

## Output

**Summary of local embedding alternatives:**

| Library | Backend | Multi-process safe? | Notes |
|---------|---------|---------------------|-------|
| fastembed (current) | ONNX native | ❌ Mutex errors | Fast but crashes |
| [Transformers.js](https://github.com/huggingface/transformers.js/) | ONNX native | ❌ Likely same issue | Same ONNX runtime |
| [TensorFlow.js USE](https://www.npmjs.com/package/@tensorflow-models/universal-sentence-encoder) | TF native | ❌ Similar issues | "Module did not self-register" errors with workers ([#2079](https://github.com/tensorflow/tfjs/issues/2079), [#8388](https://github.com/tensorflow/tfjs/issues/8388)) |
| **[Ollama](https://ollama.com/blog/embedding-models)** | HTTP API | ✅ Yes | Separate server process handles concurrency |

**Ollama is probably your best option:**
- Runs as a daemon that handles concurrency properly
- Simple HTTP API: `POST http://localhost:11434/api/embed`
- Models like `nomic-embed-text` or `mxbai-embed-large`
- You likely already have it installed for other AI work
- No native module loading in your hook processes

The tradeoff: requires Ollama running, but it's designed for concurrent access unlike in-process ONNX/TensorFlow.

Want me to update the embedding service to use Ollama instead of fastembed?
