---
id: eb143089-40c4-4766-b428-d1519c751bdc
subject: >-
  The user is asking about how to install Ollama. Let me provide clear
  installation instructions.
applies_to: global
occurred_at: '2025-12-03T11:31:24.838Z'
content_hash: 6277b145eb3e0c8b
---
## Thought

The user is asking about how to install Ollama. Let me provide clear installation instructions.

## Output

**Ollama installation:**

**macOS:**
```bash
brew install ollama
```
Or download from https://ollama.com/download

**Linux:**
```bash
curl -fsSL https://ollama.com/install.sh | sh
```

**Windows:**
Download installer from https://ollama.com/download

**Then pull an embedding model:**
```bash
ollama pull nomic-embed-text
```

**Start the server (runs on port 11434):**
```bash
ollama serve
```
(On macOS with brew, it auto-starts as a service)

**Test it:**
```bash
curl http://localhost:11434/api/embed -d '{
  "model": "nomic-embed-text",
  "input": "Hello world"
}'
```

---

Most devs doing AI work already have Ollama installed. It's pretty standard tooling at this point.

The embedding models are small (~275MB for nomic-embed-text) and the API is dead simple. Want me to update local-recall to use Ollama for embeddings?
