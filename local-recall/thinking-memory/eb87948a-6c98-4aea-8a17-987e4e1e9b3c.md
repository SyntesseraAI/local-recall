---
id: eb87948a-6c98-4aea-8a17-987e4e1e9b3c
subject: >-
  Good - I now have a clear picture. The embedding service is only used by the
  two vector stores. Let...
applies_to: global
occurred_at: '2025-12-03T11:33:45.322Z'
content_hash: 32fff2ce2c2deaa2
---
## Thought

Good - I now have a clear picture. The embedding service is only used by the two vector stores. Let me now rewrite the embedding service to use Ollama instead of fastembed.

For Ollama, the best small/fast model for embeddings is `nomic-embed-text` which is well-regarded, or `all-minilm` which is smallest. Let me go with `nomic-embed-text` as it's the recommended default and has good quality.

Actually, looking at Ollama's documentation:
- `nomic-embed-text` - 768 dimensions, 274MB - good quality
- `all-minilm` - 384 dimensions, 46MB - smallest, fast
- `mxbai-embed-large` - 1024 dimensions, 669MB - best quality

For "best & fastest while very small", I'd say `nomic-embed-text` is the best balance. It's become the de facto standard for Ollama embeddings. But since user wants "very small", maybe `all-minilm` is better.

Let me go with `nomic-embed-text` as it's the recommended one with good quality, and make the model configurable.

Now let me update the embedding service to use Ollama's HTTP API:
- POST http://localhost:11434/api/embed
- Body: { "model": "nomic-embed-text", "input": "text to embed" }

The API returns:
```json
{
  "embeddings": [[0.1, 0.2, ...]],
  "model": "nomic-embed-text"
}
```

Let me now update the embedding service:

## Output

Now I have a clear picture. Let me update the todo and rewrite the embedding service to use Ollama:
