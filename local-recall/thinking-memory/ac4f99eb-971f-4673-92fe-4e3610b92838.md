---
id: ac4f99eb-971f-4673-92fe-4e3610b92838
subject: Done! Fixed and pushed as v0.1.6.
applies_to: global
occurred_at: '2025-12-20T22:36:26.256Z'
content_hash: a1db98cb8b2930b1
---
## Thought

Done! Fixed and pushed as v0.1.6.

## Output

Fixed and pushed as `v0.1.6`. The embedding service now truncates inputs to 6000 characters (~1500 tokens) before sending to Ollama.

`★ Insight ─────────────────────────────────────`
**Embedding model context limits:** Different embedding models have varying context windows. `nomic-embed-text` is trained on 2048 tokens max. When you exceed this limit, Ollama attempts to truncate but can panic if batching/caching conflicts arise. The fix applies truncation *before* the API call rather than relying on the model's internal handling. For semantic search, the first ~6000 characters usually contain the most relevant content anyway - embeddings work better on focused content than sprawling documents.
`─────────────────────────────────────────────────`
