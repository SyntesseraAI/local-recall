---
id: 1b0a8040-f398-404c-8bfe-18eb75811564
subject: >-
  Now I understand the current implementation. It's using a single JSON file
  with a `transcripts`...
applies_to: global
occurred_at: '2025-11-30T21:22:35.601Z'
content_hash: 0430fead1ca7081a
---
Now I understand the current implementation. It's using a single JSON file with a `transcripts` record. The user wants to change this to JSONL format which would:

1. Be append-only (just add new lines)
2. Be faster (no need to parse/rewrite the entire file)
3. Be cleaner (one record per line)

However, there's a challenge: the current implementation supports:
- `removeEntry` - removing entries
- `needsProcessing` - checking if entry exists and hash matches

With JSONL, we'd need to handle these differently since JSONL is append-only. A common approach is:
1. For removals, either compact the file periodically or mark entries as deleted
2. For lookups, load all entries into memory (which is what we're doing anyway with the cache)

Let me think about the design:

Option 1: Pure append-only with periodic compaction
- Append new entries and "delete" markers
- When loading, process all lines and build the state
- Periodically compact to remove deleted entries

Option 2: Append for adds, rewrite for removes
- Append new entries
- When removing, read all, filter, rewrite

Given the use case (processed log for transcripts), Option 1 makes more sense since:
- Removes are rare (only when transcript content changes)
- We already cache the state in memory
- Appending is fast and atomic

Let me also check the test file to understand expected behavior.
