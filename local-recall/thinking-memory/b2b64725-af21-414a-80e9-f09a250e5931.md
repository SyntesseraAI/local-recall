---
id: b2b64725-af21-414a-80e9-f09a250e5931
subject: >-
  The user is asking me to extract memories from a session transcript about
  migrating from fastembed...
applies_to: global
occurred_at: '2025-12-21T17:22:30.016Z'
content_hash: ee1ba4fe3ae79014
---
## Thought

The user is asking me to extract memories from a session transcript about migrating from fastembed to Ollama for embeddings. Let me analyze what was learned/changed:

1. The migration from fastembed (BGE-small-en-v1.5, 384 dimensions) to Ollama with nomic-embed-text (768 dimensions, ~274MB)
2. Choice of nomic-embed-text as the embedding model - described as "best & fastest model possible while keeping it very small"
3. Removal of dependencies: fastembed, proper-lockfile
4. Removal of local_cache directory and references
5. Updated documentation in CLAUDE.md and docs/architecture.md
6. Successful build after migration
7. Ollama uses HTTP API at /api/embed endpoint
8. Architecture changed to use Orama (pure JavaScript) for vector store instead of SQLite
9. No more native dependencies means no mutex errors

Key memories to extract:
- The embedding model choice and its specifications
- The architecture change from SQLite to Orama
- The removal of the proper-lockfile dependency and why (ONNX mutex workarounds)
- The HTTP API approach for Ollama
- Configuration via OLLAMA_BASE_URL and OLLAMA_EMBED_MODEL
- The embedding dimension is now 768 (was 384)

Let me structure these as memories:

1. Ollama embedding migration - choice of nomic-embed-text model
2. Architecture change from SQLite to Orama for vector store
3. Removed dependencies and why (proper-lockfile was for ONNX mutex)
4. Ollama HTTP API integration details
5. Configuration environment variables

These are specific to the codebase and the decisions made during this session.

## Output

```json
{
  "memories": [
    {
      "subject": "Ollama embedding model: nomic-embed-text (768 dims, ~274MB)",
      "keywords": [
        "ollama",
        "embedding",
        "nomic-embed-text",
        "model",
        "dimensions",
        "fast",
        "small"
      ],
      "applies_to": "global",
      "content": "Migrated from fastembed (BGE-small-en-v1.5, 384 dimensions) to Ollama with nomic-embed-text model. The nomic-embed-text model was chosen as the best balance of speed and size while maintaining quality. It produces 768-dimensional embeddings and is approximately 274MB. Configured via environment variables: `OLLAMA_BASE_URL` (default: http://localhost:11434) and `OLLAMA_EMBED_MODEL` (default: nomic-embed-text)."
    },
    {
      "subject": "Vector store migrated from SQLite to Orama (pure JavaScript)",
      "keywords": [
        "vector-store",
        "orama",
        "javascript",
        "no-native-deps",
        "json-index",
        "architecture"
      ],
      "applies_to": "global",
      "content": "Vector store architecture changed from SQLite-based to Orama (pure JavaScript). Orama index files (orama-episodic-index.json, orama-thinking-index.json) are JSON-based and gitignored. This eliminates native dependency issues and mutex problems that plagued the ONNX-based fastembed implementation. Orama supports cosine distance similarity scoring with results ranked by score (0.0-1.0) and recency tie-breakers."
    },
    {
      "subject": "Removed proper-lockfile dependency (ONNX mutex workaround)",
      "keywords": [
        "proper-lockfile",
        "onnx",
        "mutex",
        "dependency",
        "removed"
      ],
      "applies_to": "global",
      "content": "Removed the proper-lockfile dependency which was only needed as a workaround for ONNX mutex issues in the fastembed implementation. With the migration to Ollama's HTTP API, there are no native dependencies and no need for file-based locking mechanisms."
    },
    {
      "subject": "Ollama HTTP API integration for embeddings",
      "keywords": [
        "ollama",
        "http-api",
        "api-embed",
        "embedding-service",
        "http"
      ],
      "applies_to": "file:src/core/embedding.ts",
      "content": "Embedding service (`src/core/embedding.ts`) now uses Ollama's HTTP API endpoint `/api/embed` instead of the fastembed library. This HTTP-based approach eliminates all native dependencies and ensures compatibility across platforms. The service sends embedding requests to the Ollama server configured via OLLAMA_BASE_URL."
    },
    {
      "subject": "Cleaned up local_cache directory references",
      "keywords": [
        "local_cache",
        "gitignore",
        "eslint-config",
        "cleanup",
        "migration"
      ],
      "applies_to": "global",
      "content": "Removed all references to local_cache directory (which stored fastembed model cache) from .gitignore and eslint.config.js. The local_cache directory is no longer needed since Ollama manages its own model cache. This simplifies project structure and removes temporary build artifacts."
    },
    {
      "subject": "Documentation updated for Ollama architecture",
      "keywords": [
        "documentation",
        "claude-md",
        "architecture-md",
        "ollama",
        "embedding-model"
      ],
      "applies_to": "global",
      "content": "Both CLAUDE.md and docs/architecture.md have been updated to reflect the new Ollama-based embedding architecture. Documentation now clearly specifies the embedding model (nomic-embed-text), its dimensions (768), size (~274MB), and the requirement to have Ollama installed and running. Migration notes have been added for users upgrading from fastembed."
    }
  ]
}
```
