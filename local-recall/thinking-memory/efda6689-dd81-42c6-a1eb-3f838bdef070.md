---
id: efda6689-dd81-42c6-a1eb-3f838bdef070
subject: >-
  Now I understand the current architecture: 1. **Embedding service**
  (`src/core/embedding.ts`): Uses...
applies_to: global
occurred_at: '2025-12-03T11:33:31.849Z'
content_hash: 17e33e16fd9952ab
---
## Thought

Now I understand the current architecture:

1. **Embedding service** (`src/core/embedding.ts`): Uses fastembed with BGE-small-en-v1.5 model (384 dimensions). Has file-based locking to prevent concurrent ONNX runtime issues.

2. **Vector store** (`src/core/vector-store.ts`): Uses Orama for vector storage/search, calls the embedding service for generating embeddings.

3. **Thinking vector store**: Similar pattern for thinking memories.

For Ollama, I need to:
1. Use a small, fast embedding model - `nomic-embed-text` is popular (768 dimensions) but `mxbai-embed-large` is also good. Actually, for "very small and fast", `all-minilm` is smaller (384 dimensions like BGE) or `nomic-embed-text` is a good balance.

Looking at Ollama's embed models:
- `nomic-embed-text` - 768 dimensions, ~274MB
- `all-minilm` - 384 dimensions, ~46MB (smallest!)
- `mxbai-embed-large` - 1024 dimensions, ~669MB

The user wants "best & fastest" while keeping it "very small". `all-minilm` is the smallest at 46MB and 384 dimensions. `nomic-embed-text` is a good balance of quality and size.

Let me check if there's a thinking vector store too.

## Output

Let me also check the thinking vector store and any other files that use embeddings:
