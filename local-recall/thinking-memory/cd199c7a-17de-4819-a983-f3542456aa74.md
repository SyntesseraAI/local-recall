---
id: cd199c7a-17de-4819-a983-f3542456aa74
subject: >-
  Excellent! Now I have comprehensive information. Let me synthesize all of this
  into a clear...
applies_to: global
occurred_at: '2025-12-03T10:37:17.221Z'
content_hash: fcd500c5f76f51d2
---
## Thought

Excellent! Now I have comprehensive information. Let me synthesize all of this into a clear recommendation analysis.

## Summary of Alternatives

### 1. Vectra
- **Type**: Pure TypeScript/JavaScript
- **Storage**: JSON files on disk
- **Pros**: No native deps, no mutex issues, simple, file-based
- **Cons**: Loads entire index into memory, not for large datasets
- **Best for**: Small to medium datasets (< 100k vectors)

### 2. LanceDB  
- **Type**: Native (Rust with Node.js bindings)
- **Storage**: Lance columnar format
- **Pros**: Fast, scalable, multimodal support, production-ready
- **Cons**: Native code (could have similar issues), different storage format
- **Best for**: Large scale, production deployments

### 3. hnswlib-node
- **Type**: Native (C++ bindings)
- **Storage**: Binary index files
- **Pros**: Fast HNSW implementation, simple API
- **Cons**: Native code (potential mutex issues)
- **Best for**: When you just need the algorithm, not full DB features

### 4. USearch
- **Type**: Native (C++ with JS bindings)
- **Storage**: Disk or memory indexes
- **Pros**: 10x faster than FAISS, memory efficient, concurrent by design
- **Cons**: Native code
- **Best for**: High-performance, large-scale search

### 5. Orama
- **Type**: Pure JavaScript
- **Storage**: In-memory with serialization
- **Pros**: Pure JS, tiny (< 2kb), full-text + vector + hybrid search
- **Cons**: Primarily in-memory (needs serialization for persistence)
- **Best for**: Browser/edge, small datasets, hybrid search needs

### 6. libsql
- **Type**: SQLite fork (native)
- **Storage**: SQLite file format


- **Pros**: Compatible with SQLite, better async, community-driven
- **Cons**: Vector support unclear, still native code
- **Best for**: If you want SQLite compatibility

## Strategic Solution Path

I see two pragmatic approaches to resolve the multi-process mutex challenges. First, pure JavaScript solutions like Vectra and Orama completely sidestep native dependency complexities. They're lightweight, memory-efficient, and perfect for smaller memory recall systems.

Alternatively, restructuring the architecture to centralize vector operations through a single HTTP daemon maintains the sqlite-vec approach while preventing concurrent access conflicts. This preserves the original design's integrity while introducing process isolation.

The key is selecting a solution that balances performance, simplicity, and reliability for the specific use case of managing around 1000 memory vectors.

## Output

Now let me compile a comprehensive analysis based on my research:

---

## Alternative Package Analysis for Local Recall

### The Core Problem
sqlite-vec uses internal C++ `std::mutex` structures that fail when multiple processes load the extension, even with file-based locking. The mutex error occurs at the native code level, not the SQLite database level.

---

## Recommended Alternatives

### ðŸ¥‡ **Option 1: Vectra** (Pure JS - Best for your use case)

| Aspect | Details |
|--------|---------|
| Type | Pure TypeScript/JavaScript |
| Storage | JSON files on disk (`index.json`) |
| Native deps | **None** - completely avoids mutex issues |
| Performance | 1-2ms queries for large indexes |
| Memory | Loads entire index into RAM |

**Pros:**
- Zero native dependencies = no mutex issues
- File-based = works with your markdown memory architecture
- MongoDB-compatible query syntax
- Cross-platform without compilation

**Cons:**
- Entire index loaded into memory (fine for ~1000 memories)
- Not suited for millions of vectors

**Verdict:** Perfect fit for local-recall's scale. ~1000 memories easily fits in memory.

GitHub: [Stevenic/vectra](https://github.com/Stevenic/vectra)

---

### ðŸ¥ˆ **Option 2: Orama** (Pure JS - Modern alternative)

| Aspect | Details |
|--------|---------|
| Type | Pure JavaScript |
| Size | < 2kb minified |
| Features | Full-text + vector + hybrid search |
| Native deps | **None** |

**Pros:**
- Pure JS, no native code
- Hybrid search (combine text + vector)
- Plugin system for embeddings
- Excellent documentation

**Cons:**
- In-memory (needs serialization strategy)
- Newer project

**Verdict:** Great if you want hybrid full-text + vector search.

GitHub: [askorama/orama](https://github.com/askorama/orama)

---

### ðŸ¥‰ **Option 3: USearch** (Native but designed for concurrency)

| Aspect | Details |
|--------|---------|
| Type | Native (C++) with Node.js bindings |
| Performance | "10x faster than FAISS" |
| Threading | "Concurrent by design" |
| Storage | Disk or memory indexes |

**Pros:**
- Explicitly designed for concurrent access
- Very fast, memory efficient
- Supports disk-based indexes (no full memory load)

**Cons:**
- Still native code (lower risk but not zero)
- Newer Node.js bindings

**Verdict:** If you need maximum performance and scale.

GitHub: [unum-cloud/usearch](https://github.com/unum-cloud/usearch)

---

### ðŸ”§ **Option 4: Restore HTTP Daemon Architecture** (Keep sqlite-vec)

This was your original design per CLAUDE.md:
```
Hook Process â†’ HTTP request â†’ Daemon (owns sqlite-vec) â†’ response
```

**Pros:**
- No code migration needed
- sqlite-vec works fine when single process owns it
- Already documented in CLAUDE.md

**Cons:**
- Requires daemon to be running
- HTTP overhead (minimal)

---

## My Recommendation

For local-recall specifically, I'd recommend **Vectra**:

1. **Scale fits**: Your ~1000 memories easily fit in memory
2. **No native deps**: Completely eliminates mutex issues
3. **File-based**: Aligns with your markdown memory architecture
4. **Simple migration**: Just replace VectorStore with Vectra index

```typescript
// Example Vectra usage
import { LocalIndex } from 'vectra';

const index = new LocalIndex(path.join(memoryDir, 'vectra-index'));
await index.createIndex();
await index.insertItem({ id: memory.id, vector: embedding, metadata: { subject, ... } });
const results = await index.queryItems(queryEmbedding, 10);
```

Would you like me to implement a proof-of-concept migration to Vectra?

---

**Sources:**
- [Vectra GitHub](https://github.com/Stevenic/vectra)
- [Orama GitHub](https://github.com/askorama/orama)
- [USearch GitHub](https://github.com/unum-cloud/usearch)
- [LanceDB GitHub](https://github.com/lancedb/lancedb)
- [hnswlib-node GitHub](https://github.com/yoshoku/hnswlib-node)
- [VectorDB.js](https://vectordbjs.themaximalist.com/)
- [RxDB Vector Database Article](https://rxdb.info/articles/javascript-vector-database.html)
