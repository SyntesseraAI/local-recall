---
id: 6f175590-3afc-4096-8ec7-ea416d9187b0
subject: >-
  The user found the root cause. The Ollama logs show: 1. `requested context
  size too large for...
applies_to: global
occurred_at: '2025-12-20T22:35:27.352Z'
content_hash: ca99e4f0f438cfcc
---
## Thought

The user found the root cause. The Ollama logs show:

1. `requested context size too large for model` - num_ctx=8192 but n_ctx_train=2048
2. `truncating input prompt` - limit=2048 but prompt=2420 tokens
3. **Panic**: `caching disabled but unable to fit entire input in a batch`

The `nomic-embed-text` model has a max context of 2048 tokens, but we're sending inputs with 2420+ tokens. The model panics because it can't fit the input.

Yes, we need to either:
1. Chunk inputs that are too long
2. Truncate inputs to fit within 2048 tokens

Looking at the embedding.ts, there's no truncation happening. We need to add it.

For nomic-embed-text, the context window is 2048 tokens. A rough estimate is ~4 characters per token, so we should limit input to around 6000-7000 characters to be safe (with some margin).

Let me add truncation to the embedding service.

## Output

Yes! The `nomic-embed-text` model has a **2048 token context limit**, and we're sending inputs with 2420+ tokens. The model panics when input exceeds the limit.

Let me add input truncation:
