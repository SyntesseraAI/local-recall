---
id: bbde60e9-193e-47c0-baf5-8f6b3dc34905
subject: >-
  The user is asking me to extract memories from a transcript about updating
  documentation for the...
applies_to: global
occurred_at: '2025-12-21T19:16:14.252Z'
content_hash: 7ac3c3f9e2992a99
---
## Thought

The user is asking me to extract memories from a transcript about updating documentation for the local-recall project. Let me analyze what valuable information was discovered or documented during this session.

Key things that happened:
1. The user asked to update documentation and README for new users
2. The assistant reviewed the codebase, git history, and memories
3. Multiple documentation files were updated to reflect the current state (migration from SQLite to Orama, from fastembed to Ollama)
4. The assistant discovered and fixed several documentation gaps:
   - README.md had wrong MCP tools listed
   - Docs referenced old SQLite/fastembed architecture
   - Configuration options were outdated
   - Log level defaults were wrong
   - MCP port was wrong

5. A discussion about isolation and multi-instance scenarios
6. Discussion about storage architecture for distributed agents

Key discoveries/insights from the session:
1. Local-recall migrated from SQLite + fastembed to Orama + Ollama
2. The hook system is now unified (single UserPromptSubmit hook instead of separate episodic/thinking)
3. Stop hook is currently disabled (memory extraction via daemon)
4. Configuration defaults: episodicMinSimilarity 0.5, thinkingMinSimilarity 0.5, LOG_LEVEL error, MCP port 7847
5. Each Claude Code instance spawns its own MCP server via stdio
6. Multiple instances in same folder have race conditions on Orama indexes
7. Thinking memories were added as new feature (thought + output pairs)
8. The daemon processes transcripts every 5 minutes
9. Proposed architecture: embedding files stored alongside memories for distributed/no-embedding modes

What should be extracted as memories?

DO extract:
- Architecture decisions (Orama + Ollama, embedding files concept)
- Multi-instance behavior and limitations
- Current configuration defaults that differ from documentation
- New feature: thinking memories format
- Daemon architecture (5 minute processing cycle)


- Migration from SQLite+fastembed to Orama+Ollama
- Proposed embedding file sidecar pattern

I'll focus on extracting key architectural insights and practical implementation details. The migration represents a significant shift in the project's core technology stack, moving from SQLite and fastembed to Orama and Ollama. This change impacts storage, embedding, and overall system performance. The embedding file sidecar pattern introduces a novel approach to distributed memory management, allowing more flexible deployment scenarios.

The multi-instance behavior reveals critical limitations, particularly race conditions when multiple instances interact with Orama indexes in the same folder. This insight is crucial for developers planning complex deployments.

Configuration defaults provide essential context for new users, highlighting specific settings like similarity thresholds, log levels, and communication port. The thinking memories feature adds a new dimension to memory capture, storing both thought processes and their corresponding outputs.

## Output

```json
{
  "memories": [
    {
      "subject": "Architecture migration: SQLite+fastembed → Orama+Ollama",
      "keywords": ["orama", "ollama", "nomic-embed-text", "embeddings", "vector-store", "migration", "architecture"],
      "applies_to": "global",
      "content": "Local-recall migrated from SQLite-based storage with fastembed (BGE-small-en-v1.5, 384 dimensions) to Orama-based vector store with Ollama (nomic-embed-text, 768 dimensions). This is a fundamental architectural change:\n\n- **Old**: SQLite database with fastembed for local embeddings\n- **New**: Orama pure JavaScript vector store with Ollama server for embeddings\n- **Why**: Orama has no native dependencies (no mutex/process isolation issues)\n- **Index files**: `orama-episodic-index.json` and `orama-thinking-index.json` are JSON-based and gitignored\n- **Model**: nomic-embed-text (768 dimensions, ~274MB)\n- **Ollama requirement**: Must be running and accessible at `http://localhost:11434` (or `OLLAMA_BASE_URL`)"
    },
    {
      "subject": "Hook system unified: Single UserPromptSubmit for episodic + thinking memories",
      "keywords": ["hooks", "user-prompt-submit", "episodic", "thinking", "unified", "configuration"],
      "applies_to": "global",
      "content": "The hook system was refactored to use a single unified `UserPromptSubmit` hook instead of separate hooks for episodic and thinking memories. Configuration controls which memory types are retrieved:\n\n- **Single hook**: `user-prompt-submit.js` handles both memory types\n- **Episodic control**: `episodicEnabled`, `episodicMaxTokens`, `episodicMinSimilarity`\n- **Thinking control**: `thinkingEnabled`, `thinkingMaxTokens`, `thinkingMinSimilarity`\n- **Skip internal prompts**: Hooks skip prompts containing `[LOCAL_RECALL_INTERNAL]` (used for memory extraction)\n- **Stop hook**: Currently disabled - memory extraction is handled by the MCP server daemon (5-minute cycle)"
    },
    {
      "subject": "Thinking memories format: Thought + Output paired storage",
      "keywords": ["thinking-memory", "thought-output", "reasoning", "extraction", "format"],
      "applies_to": "global",
      "content": "Thinking memories capture Claude's internal reasoning paired with its output response. Format:\n\n```markdown\n---\nid: uuid\nsubject: First sentence from thought\napplies_to: global\noccurred_at: ISO-8601\ncontent_hash: SHA-256 prefix\n---\n\n## Thought\n[Claude's reasoning/thinking block]\n\n## Output\n[The text response that followed]\n```\n\n- **Extraction**: Daemon extracts thinking blocks from transcripts (20 parallel workers)\n- **Storage**: Each thought+output pair stored as single memory file\n- **Tracking**: `thinking-processed-log.jsonl` tracks which transcripts were processed\n- **Reset**: Delete `thinking-processed-log.jsonl` and `orama-thinking-index.json` to reprocess all transcripts"
    },
    {
      "subject": "Configuration defaults and environment variables",
      "keywords": ["configuration", "environment-variables", "defaults", "settings", ".local-recall.json"],
      "applies_to": "global",
      "content": "Default configuration values for local-recall:\n\n**Memory settings**:\n- `episodicMinSimilarity`: 0.5 (50% similarity threshold)\n- `thinkingMinSimilarity`: 0.5 (50% similarity threshold)\n- `episodicMaxTokens`: 1000\n- `thinkingMaxTokens`: 1000\n- `maxMemories`: 1000\n- `hooks.maxContextMemories`: 10 (episodic memories in session start)\n\n**Logging and infrastructure**:\n- `LOG_LEVEL`: `error` (default, not debug as some old docs stated)\n- `indexRefreshInterval`: 300 seconds (5 minutes)\n- `OLLAMA_BASE_URL`: `http://localhost:11434`\n- `OLLAMA_EMBED_MODEL`: `nomic-embed-text`\n\n**MCP Server**: Listens on port 7847 (not 3000)"
    },
    {
      "subject": "Multi-instance race conditions on shared codebase",
      "keywords": ["concurrency", "multi-instance", "race-condition", "isolation", "stdio", "daemon"],
      "applies_to": "global",
      "content": "When multiple Claude Code instances run on the SAME codebase folder:\n\n**Problems**:\n- Multiple MCP server processes write to `orama-episodic-index.json` simultaneously → lost updates\n- Multiple daemons may process same transcript → duplicate memories\n- `recall.log` gets duplicate entries\n- Orama index corruption possible\n\n**Why it happens**:\n- Each Claude instance spawns its own MCP server via stdio\n- No mutex/file locking between processes\n- Orama indexes are JSON files (not atomic operations)\n\n**Safe scenario**: Each project in separate folder has own isolated `local-recall/` directory - no conflicts\n\n**Mitigation**: Consider implementing file locking or switching to single shared daemon with TCP instead of stdio (would require architecture change)"
    },
    {
      "subject": "Daemon architecture: Transcript processing every 5 minutes",
      "keywords": ["daemon", "background-processing", "transcript", "interval", "mcp-server", "memory-extraction"],
      "applies_to": "global",
      "content": "The MCP server includes a background daemon that processes transcripts asynchronously:\n\n**Behavior**:\n- Runs every 5 minutes (configurable via `indexRefreshInterval`)\n- Syncs transcripts from Claude cache: `~/.claude/projects/<project>/transcripts/`\n- Extracts memories using `claude -p` command\n- Tracks processed transcripts with content hash in `processed-log.jsonl`\n- When transcript changes → deletes old memories, creates new ones\n\n**Why**: Keeps memory extraction out of the hot path (hooks stay fast)\n\n**File tracking**: Uses `episodic-processed-log.jsonl` and `thinking-processed-log.jsonl` to avoid reprocessing"
    },
    {
      "subject": "Proposed: Embedding file sidecar pattern for distributed agents",
      "keywords": ["embeddings", "sidecar-files", "distributed", "no-embeddings-mode", "cloud-agents", "git-tracked"],
      "applies_to": "global",
      "content": "To support cloud agents that can create memories but lack Ollama access:\n\n**Pattern**: Store embedding vectors alongside memory files\n\n```\nepisodic-memory/\n├── abc123.md              ← Memory content (git tracked)\n└── abc123.embedding.json  ← Vector embedding (git tracked)\n```\n\n**Embedding file format**:\n```json\n{\n  \"model\": \"nomic-embed-text\",\n  \"dimensions\": 768,\n  \"created_at\": \"ISO-8601\",\n  \"vector\": [0.1, -0.2, ..., 0.5]\n}\n```\n\n**Two modes**:\n1. **Standard mode** (local dev): Try loading from disk, if missing use Ollama to generate\n2. **No-embeddings mode** (cloud agents): Load existing embeddings from disk only, don't create new ones\n\n**Benefit**: Cloud agents can create memories via MCP without Ollama, local dev auto-generates missing embeddings\n\n**Implementation**: Update vector-store to check for `.embedding.json` sidecar before calling Ollama"
    }
  ]
}
```
