---
id: a3b02597-7df4-41d0-adca-0e5a05505981
subject: Vector-based semantic search implementation for memories
keywords:
  - vector-store
  - embeddings
  - semantic-search
  - bghe-small
  - similarity
  - memory-search
  - fastembed
applies_to: 'area:memory-search'
occurred_at: '2025-12-02T01:00:56.407Z'
content_hash: 6c69919ff453c62a
---
# Vector-Based Semantic Search for Memories

## Overview

The project implements semantic search on memories using vector embeddings. This allows finding relevant memories even when keywords don't match exactly.

## Implementation Details

### Embedding Model
- **Model**: BGE-small-en-v1.5 (Baidu General Embedding)
- **Library**: `fastembed` (Rust-based, fast embeddings)
- **Size**: ~133MB, downloaded on first use
- **Cache Location**: `local_cache/fast-bge-small-en-v1.5/`

### Vector Store
- Memories are converted to vectors and stored in a vector database
- Search queries are also vectorized and compared for semantic similarity
- Returns results ranked by cosine similarity score

### Components

1. **Embedding Generation** (`src/core/embedding.ts`):
   - Converts memory content to vector embeddings
   - Stores vectors alongside memory metadata

2. **Vector Store** (`src/core/vector-store.ts`):
   - Manages storage and retrieval of vectors
   - Implements similarity search
   - Handles vector indexing for performance

## Advantages Over Keyword Search

- Finds conceptually related memories even with different wording
- Better handling of synonyms and related concepts
- More intuitive search experience for users

## Performance Notes

- First run: ~30-60 seconds for model download and initialization
- Subsequent runs: Fast, model loaded from cache
- Vector operations are efficient with small model size
