---
id: 492b2bb4-0c86-43aa-9824-481ac533be17
subject: Nomic-embed-text model has 2048 token context limit
keywords:
  - ollama
  - nomic-embed-text
  - embedding
  - context limit
  - truncation
  - 2048 tokens
applies_to: 'file:src/core/embedding.ts'
occurred_at: '2025-12-21T19:00:26.631Z'
content_hash: 84758c0a366d4815
---
The `nomic-embed-text` embedding model used for vector search has a **2048 token context limit** (trained on max 2048 tokens). When input text exceeds this, Ollama's subprocess panics with EOF errors and internal routing failures.

The fix is to truncate input text to ~6000 characters (~1500 tokens, leaving safety margin) before sending to Ollama's `/api/embeddings` endpoint. This prevents model context overflow while maintaining semantic quality of embeddings.

Error pattern: "requested context size too large for model num_ctx=8192 n_ctx_train=2048"
