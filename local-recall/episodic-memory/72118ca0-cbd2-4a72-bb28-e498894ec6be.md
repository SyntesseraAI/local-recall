---
id: 72118ca0-cbd2-4a72-bb28-e498894ec6be
subject: Nomic-embed-text model has 2048 token context limit
keywords:
  - ollama
  - embedding
  - nomic-embed-text
  - context window
  - truncation
applies_to: 'file:src/core/embedding.ts'
occurred_at: '2025-12-21T19:00:39.886Z'
content_hash: af4ab2aecbcae285
---
The `nomic-embed-text` embedding model is trained with a maximum context window of 2048 tokens. Sending inputs exceeding this limit causes Ollama to fail with internal subprocess communication errors ("requested context size too large for model").

Fix: Truncate all inputs to 6000 characters (~1500 tokens) before sending to Ollama. This ensures the embedding service stays within safe margins and avoids triggering Ollama's panic behavior on oversized requests.

Note: Different embedding models have different context limits - always check the model's specifications.
