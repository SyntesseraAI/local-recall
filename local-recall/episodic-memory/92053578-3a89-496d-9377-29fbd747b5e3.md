---
id: 92053578-3a89-496d-9377-29fbd747b5e3
subject: >-
  Embedding models use fastembed library with BGE-small-en-v1.5, model cached in
  local_cache/
keywords:
  - embedding
  - fastembed
  - bge-small
  - model-caching
  - vector-store
  - semantic-search
applies_to: 'area:embedding'
occurred_at: '2025-12-01T23:15:35.032Z'
content_hash: b213557e7ac7157b
---
# Embedding Model Configuration

Local Recall uses the `fastembed` library with the BGE-small-en-v1.5 model for semantic search capabilities.

## Model Details

- **Library**: fastembed (Python-based via node bindings)
- **Model**: BGE-small-en-v1.5
- **Size**: ~133MB
- **Purpose**: Semantic similarity search for memory retrieval

## Caching

The model is automatically cached in the `local_cache/` directory:
- First download takes 30-60 seconds
- Subsequent runs load from cache
- Cache location: `local_cache/fast-bge-small-en-v1.5/`

## Common Issues

If you see "Tokenizer file not found at local_cache/fast-bge-small-en-v1.5/tokenizer.json":
- The cache was corrupted or download was interrupted
- Solution: `rm -rf local_cache/fast-bge-small-en-v1.5*` then run again

## Integration

The vector-store module should use fastembed to create embeddings for memory content, enabling semantic similarity matching beyond keyword-based search.
