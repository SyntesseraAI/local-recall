---
id: 6b2f2060-4922-47af-b122-c820962c8b76
subject: Embedding model setup uses fastembed with BGE-small-en-v1.5
keywords:
  - embeddings
  - vector-store
  - fastembed
  - bge-small
  - semantic-search
  - python
applies_to: 'area:vector-store'
occurred_at: '2025-12-02T07:19:18.638Z'
content_hash: 128f70d238b24519
---
# Embedding Model Configuration

Local Recall is implementing semantic search using embeddings with the following setup:

## Model Details

- **Library**: `fastembed` (Python)
- **Model**: BGE-small-en-v1.5
- **Size**: ~133MB
- **Cache**: Stored in `local_cache/fast-bge-small-en-v1.5/`

## Node.js Integration

To use this in Node.js, we need to:
1. Install Python and fastembed library
2. Create a Python wrapper script that can be called from Node.js
3. Handle JSON communication between Node.js and Python process
4. Cache embeddings to avoid repeated computation

## Performance

- First run downloads the model (~30-60 seconds)
- Subsequent runs use cached model (~133MB locally)
- Model caches in `local_cache/` directory

## Relevant Files

- `src/core/embedding.ts` - Node.js embedding interface
- `src/core/vector-store.ts` - Vector storage and similarity search
