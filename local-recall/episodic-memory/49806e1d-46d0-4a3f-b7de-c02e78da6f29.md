---
id: 49806e1d-46d0-4a3f-b7de-c02e78da6f29
subject: Ollama is the recommended solution for multi-instance Claude support
keywords:
  - ollama
  - embedding
  - daemon
  - multi-instance
  - nomic-embed-text
  - http
  - api
applies_to: global
occurred_at: '2025-12-21T18:26:49.611Z'
content_hash: bc6fc8aba4c44909
---
Ollama provides a shared embedding server that eliminates ONNX mutex issues by centralizing the model loading. Multiple Claude instances can call the same Ollama daemon via HTTP API instead of each loading their own ONNX runtime.

Ollama advantages:
- Single model instance shared across all processes
- HTTP API (no native dependencies in hooks)
- Runs as a background service
- Supports the nomic-embed-text model (768 dimensions, ~274MB)
- Easy installation via package managers (brew on macOS, apt on Linux)
- Auto-starts as a service on macOS

Setup:
1. Install: `brew install ollama` (macOS) or download from ollama.com
2. Pull model: `ollama pull nomic-embed-text`
3. Runs on port 11434 by default

Hooks would call Ollama's HTTP embedding endpoint instead of loading fastembed directly, preventing concurrent ONNX initialization issues.
