---
id: 34d63c04-ebae-4dba-a2fc-99c93103c4e0
subject: Embedding model setup and fastembed configuration for semantic search
keywords:
  - embedding
  - fastembed
  - bge-small
  - semantic-search
  - vector-store
  - model-cache
  - tokenizer
applies_to: 'area:vector-store'
occurred_at: '2025-12-02T02:24:12.745Z'
content_hash: d8390d182da19e12
---
# Embedding Model Configuration

Local Recall uses the `fastembed` library with the BGE-small-en-v1.5 model for semantic search capabilities.

## Model Details
- Model: BGE-small-en-v1.5 from Hugging Face
- Library: `fastembed` Python package
- Model size: ~133MB
- Cache location: `local_cache/`

## Important Notes

1. **First Run**: Initial startup downloads the model (30-60 seconds) and caches it
2. **Tokenizer**: The tokenizer file is downloaded separately and stored at `local_cache/fast-bge-small-en-v1.5/tokenizer.json`
3. **Cache Corruption**: If tokenizer file errors occur, delete `local_cache/fast-bge-small-en-v1.5*` directories to force re-download

## Implementation

The embedding model is integrated into `src/core/vector-store.ts` for semantic search of memory content. This enables finding contextually relevant memories beyond keyword matching.
