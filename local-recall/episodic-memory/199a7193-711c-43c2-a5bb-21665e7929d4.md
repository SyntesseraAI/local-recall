---
id: 199a7193-711c-43c2-a5bb-21665e7929d4
subject: >-
  Ollama is the recommended solution for local embeddings with multiple
  processes
keywords:
  - ollama
  - embedding
  - http-api
  - multi-process
  - safe
  - nomic-embed-text
applies_to: global
occurred_at: '2025-12-21T18:24:57.085Z'
content_hash: 49c46d81ea987e65
---
Ollama (https://ollama.com) is a standalone embedding server that solves the ONNX mutex problem. It runs as a single system-wide service (one ONNX instance), and multiple processes/Claude instances can call it via HTTP API on port 11434. The mutex issue only occurs during ONNX initialization - a shared daemon means ONNX loads once, then many clients use it safely. Installation: `brew install ollama` (macOS), or download from ollama.com. Pull embedding model with `ollama pull nomic-embed-text`. CLAUDE.md already documents Ollama as the embedding service with env vars OLLAMA_BASE_URL and OLLAMA_EMBED_MODEL.
