---
id: 28aa99a8-e10d-493b-8e77-e04ea316a6a8
subject: >-
  Memory extraction uses token counting to condense transcripts before
  processing
keywords:
  - token-counting
  - transcript-condensing
  - memory-extraction
  - performance
  - openai-tokenizer
applies_to: 'area:memory-extraction'
occurred_at: '2025-12-01T15:53:35.620Z'
content_hash: 3e0361fed87a0052
---
# Transcript Condensing with Token Counting

The memory extraction process uses token counting to condense transcripts before feeding them to Claude for memory extraction.

## Implementation

- Located in `src/utils/transcript.ts`
- Uses `js-tiktoken` (OpenAI tokenizer) to count tokens in transcript content
- Implements a `condenseTranscript()` function that:
  1. Counts tokens in the original transcript
  2. Condenses the transcript if it exceeds a threshold (e.g., 6000 tokens)
  3. Removes less important events (like Tool outputs) first
  4. Preserves User and Assistant messages
  5. Returns the condensed transcript and token count

## Token Counting Library

- Package: `js-tiktoken`
- Provides OpenAI-compatible token counting
- Allows accurate estimation of API costs and prompt sizes
- Used to optimize transcript processing before sending to Claude

## Why This Matters

- Keeps API costs down by avoiding unnecessary tokens
- Allows processing of long sessions without hitting token limits
- Still preserves the most important information (user requests and assistant responses)
- Maintains efficiency in the memory extraction pipeline
