---
id: 05abb5f8-b50f-45bd-a87d-48e20cd076ec
subject: >-
  Ollama is preferred for embedding inference - standalone daemon with HTTP API
  for multi-process safety
keywords:
  - ollama
  - embedding
  - daemon
  - http
  - installation
  - alternatives
  - nomic-embed-text
applies_to: global
occurred_at: '2025-12-03T11:35:39.170Z'
content_hash: eae32933ec087a01
---
Ollama is a standalone embedding server that solves the ONNX concurrency issue. Installation: macOS via `brew install ollama` or from https://ollama.com/download, Linux via shell script at https://ollama.com/install.sh, Windows via installer. Pull embedding model with `ollama pull nomic-embed-text`. Server runs on port 11434. Multiple Claude instances can safely query it via HTTP API without mutex conflicts. This is superior to embedding multiple ONNX instances.
