---
id: 56fa5455-60b4-41d6-a165-3e29540c45bb
subject: BGE-small-en-v1.5 embedding model for semantic search in Local Recall
keywords:
  - embedding
  - semantic-search
  - bge-small
  - fastembed
  - vector-store
  - model-cache
applies_to: 'area:vector-search'
occurred_at: '2025-12-01T21:41:32.228Z'
content_hash: 0a4853cda77d280c
---
## BGE-small-en-v1.5 Embedding Model

Local Recall uses the BGE-small-en-v1.5 model from the fastembed library for semantic search capabilities.

### Model Details

- **Model name**: BGE-small-en-v1.5
- **Library**: fastembed (Python library for fast embeddings)
- **Size**: ~133MB
- **Cache location**: `local_cache/` directory in the project root
- **Performance**: Fast inference suitable for keyword extraction and semantic matching

### How It's Used

1. **First run**: Model automatically downloads (~30-60 seconds initial startup)
2. **Subsequent runs**: Loads from cache in `local_cache/`
3. **Purpose**: Generate vector embeddings for semantic search of memories
4. **Integration**: Used in `src/core/vector-store.ts` for embedding operations

### Important Notes

- The model is downloaded automatically on first use
- Cache is stored locally and should be preserved
- If cache is corrupted, delete `local_cache/fast-bge-small-en-v1.5*` to force re-download
- Model is accessed via Python integration with the Node.js codebase
- Embeddings enable semantic search beyond simple keyword matching
