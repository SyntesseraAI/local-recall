---
id: a132d993-26c9-4f43-806d-5babf52fbbcd
subject: Transcript condensing implementation for memory extraction
keywords:
  - transcript-condensing
  - claude-api
  - summarization
  - token-efficiency
  - memory-extraction
  - haiku-model
applies_to: 'area:memory-extraction'
occurred_at: '2025-12-02T01:00:56.407Z'
content_hash: 051ea31ace52cb61
---
# Transcript Condensing for Memory Extraction

## Purpose

Transcripts can be very large (thousands of events), making it inefficient to send the entire transcript to Claude for memory extraction. Condensing reduces the transcript to key events while preserving important information.

## Implementation Approach

Use Claude Haiku (lightweight model) to condense transcripts:
- Input: Full transcript in JSONL format
- Process: Haiku analyzes the transcript and outputs a condensed version
- Output: Condensed transcript that preserves key decisions, learnings, and actions

## Key Considerations

1. **Model Selection**: Use Haiku for cost efficiency - transcript condensing doesn't need a powerful model
2. **Preservation**: Ensure condensed transcript still captures:
   - User requests and decisions
   - Solutions implemented
   - Bugs discovered and fixed
   - Architecture changes
   - Commands executed
3. **Format**: Output should be parseable (likely back to JSONL or markdown format)
4. **Token Counting**: Monitor token usage since this runs on every transcript batch

## Related Code

See `src/core/memory-extractor.ts` for the condensing implementation. The condensed transcript is then used for memory extraction with a more capable model (Claude Sonnet or similar).
