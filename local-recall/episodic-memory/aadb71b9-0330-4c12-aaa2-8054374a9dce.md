---
id: aadb71b9-0330-4c12-aaa2-8054374a9dce
subject: Memory extraction should use transcript condensing to reduce token usage
keywords:
  - memory-extraction
  - transcript-condensing
  - token-efficiency
  - llm-processing
  - user-prompt-submit
applies_to: 'file:src/hooks/user-prompt-submit.ts'
occurred_at: '2025-12-02T01:28:45.577Z'
content_hash: c10dae3d11bb8f34
---
# Transcript Condensing for Memory Extraction

When extracting memories from transcripts using Claude Haiku, the transcript should be condensed first to reduce token usage. This is important because:

1. **Token efficiency**: Raw transcripts can be very large (thousands of tokens), which increases API costs
2. **Context preservation**: Condensing should maintain important context while removing redundancy
3. **Implementation**: The transcript-condenser module should be used before passing transcripts to the extraction API

Currently, the `user-prompt-submit.ts` hook extracts keywords from prompts but doesn't handle full transcript memory extraction. Full transcript memory extraction happens in the MCP server daemon which handles this asynchronously.

Refer to commit: `8c4fbf5 feat: implement transcript condensing for memory extraction`
