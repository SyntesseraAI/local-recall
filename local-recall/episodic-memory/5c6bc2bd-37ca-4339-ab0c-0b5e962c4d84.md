---
id: 5c6bc2bd-37ca-4339-ab0c-0b5e962c4d84
subject: Ollama provides single shared embedding daemon for multiple Claude instances
keywords:
  - ollama
  - embedding
  - daemon
  - http-api
  - local
  - architecture
applies_to: global
occurred_at: '2025-12-21T19:19:54.289Z'
content_hash: 766d7fadf905f7c5
---
Ollama is an npm-installable embedding server that avoids ONNX concurrency issues by running as a single daemon process. Installation is simple:

**macOS**: `brew install ollama` or download from https://ollama.com/download
**Linux**: `curl -fsSL https://ollama.com/install.sh | sh`
**Windows**: Download installer from https://ollama.com/download

**Setup**: Pull embedding model: `ollama pull nomic-embed-text`, then start: `ollama serve` (runs on port 11434)

**Why this solves the problem**: One Ollama daemon handles all embedding requests via HTTP API, regardless of how many Claude instances run. The ONNX model loads once in the daemon process, not once per Claude instance. Multiple clients can safely query the already-loaded model without mutex conflicts.
