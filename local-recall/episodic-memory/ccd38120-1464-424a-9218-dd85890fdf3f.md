---
id: ccd38120-1464-424a-9218-dd85890fdf3f
subject: >-
  Vector store implementation planned for memory search - uses fastembed
  BGE-small model
keywords:
  - vector-store
  - embedding
  - search
  - fastembed
  - bge-small
  - semantic-search
applies_to: 'area:vector-store'
occurred_at: '2025-12-02T06:44:10.733Z'
content_hash: 755c8de7e2387995
---
# Vector Store Implementation for Memory Search

## Overview

The local-recall project is implementing semantic search capabilities using vector embeddings. This replaces or supplements the existing fuzzy keyword matching.

## Technology Stack

- **Embedding Library:** `fastembed` (Python library with Node.js bindings)
- **Model:** BGE-small-en-v1.5 (~133MB)
- **Cache Location:** `local_cache/` (auto-downloaded on first use)
- **First Run:** Takes 30-60 seconds while model downloads, cached thereafter

## Files Created/Modified

- `src/core/vector-store.ts` - New vector store implementation
- `src/core/embedding.ts` - New embedding utility module
- Dependencies added to `package.json`

## Architecture

The vector store:
1. Converts memory content into embeddings
2. Stores embeddings alongside memories (index.json)
3. Accepts query strings
4. Returns semantically similar memories based on cosine similarity

## Integration Points

- Search functionality should use both keyword matching and semantic search
- MCP server tools will expose vector search capabilities
- Memory extraction from transcripts can leverage semantic understanding
