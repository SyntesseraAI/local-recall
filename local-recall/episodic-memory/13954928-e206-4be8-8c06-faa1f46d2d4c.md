---
id: 13954928-e206-4be8-8c06-faa1f46d2d4c
subject: 'Embedding model implementation decision: fastembed vs external API'
keywords:
  - embedding
  - vector-store
  - fastembed
  - semantic-search
  - model-management
applies_to: 'area:embedding'
occurred_at: '2025-12-02T01:04:49.159Z'
content_hash: 3736159c44567529
---
# Embedding Model Implementation

The project uses the `fastembed` library with BGE-small-en-v1.5 model for semantic search capabilities.

## Why fastembed?

- **Local execution**: Embeddings run locally, no API calls or network dependencies
- **Fast**: Despite the name, it's designed for reasonable performance on local hardware
- **Model size**: BGE-small-en-v1.5 is ~133MB, manageable for local storage
- **Self-contained**: Handles model downloading and caching automatically

## Caching

The model is cached in `local_cache/` directory on first download. Subsequent runs load from cache. First initialization may take 30-60 seconds.

## Relevance

This is used by the vector-store component for semantic search of memories, complementing the keyword-based index approach.
