---
id: a6dab4dd-3168-4bf2-ac22-173c6e94d274
subject: >-
  Ollama is recommended solution for embedding inference across multiple Claude
  instances
keywords:
  - ollama
  - embedding
  - server
  - daemon
  - inference
  - multi-instance
applies_to: global
occurred_at: '2025-12-21T19:19:42.881Z'
content_hash: 87965dc8b4d3ea31
---
Ollama provides a shared embedding inference daemon that multiple Claude instances can call via HTTP API. Unlike in-process ONNX loading, a single Ollama server instance can safely handle concurrent embedding requests from multiple Claude processes without mutex issues. Ollama runs on port 11434 by default and supports the nomic-embed-text model (768 dimensions, ~274MB). Installation: macOS (brew install ollama), Linux (curl install script), Windows (installer from ollama.com). Must pull model with 'ollama pull nomic-embed-text' before use.
