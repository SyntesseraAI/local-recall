---
id: 479462f7-df75-4688-b9f0-765a9a33850d
subject: >-
  Memory extraction should deduplicate and condense transcript content before
  saving
keywords:
  - transcript
  - memory-extraction
  - deduplication
  - condensing
  - content-hash
  - occurred-at
  - duplicate-detection
applies_to: 'area:memory-extraction'
occurred_at: '2025-12-02T02:36:05.507Z'
content_hash: 84d0ff62455d2897
---
# Memory Extraction Deduplication and Condensing

## Problem

Transcripts grow very large over time and contain repeated information. When extracting memories from transcripts, the system needs to:
1. Avoid creating duplicate memories for the same information appearing multiple times
2. Condense repetitive transcript content before storing it

## Solution

The memory extraction process should:

1. **Condense transcript content** - Summarize repetitive or redundant information in the transcript before memory extraction
2. **Use deduplication fields** - The memory system already has fields designed for this:
   - `occurred_at`: ISO-8601 timestamp of when the event occurred
   - `content_hash`: SHA-256 prefix hash of the memory content
3. **Check for duplicates** - Use `findDuplicate(occurredAt, contentHash)` before creating memories
4. **Return existing on duplicate** - If a duplicate is detected, return the existing memory ID instead of creating a new one

## Implementation Details

- The `MemoryManager.createMemory()` method already handles duplicate detection internally
- Transcript condensing reduces noise and improves memory quality
- This prevents the same information from being saved multiple times as transcripts accumulate
