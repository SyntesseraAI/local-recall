---
id: 014fd3f4-f057-464c-814b-2f1282e15ecf
subject: nomic-embed-text model has 2048 token context limit
keywords:
  - ollama
  - embedding
  - nomic-embed-text
  - context-limit
  - truncation
  - error-handling
applies_to: 'file:src/core/embedding.ts'
occurred_at: '2025-12-20T22:38:28.607Z'
content_hash: 37806e2b9514a90a
---
The `nomic-embed-text` embedding model used for vector search has a hard context limit of 2048 tokens. When inputs exceed this limit, Ollama logs a warning and may experience crashes or EOF errors.

The solution is to truncate embedding inputs to approximately 6000 characters (~1500 tokens, well under the limit) BEFORE sending to Ollama's API. This prevents context overflow errors and ensures consistent embedding generation.

Error signature: `Ollama embed failed: 500 - {"error":"do embedding request: Post ... EOF"}`
