---
id: fe161814-3553-47ac-b228-d100e5f7cae2
subject: Embedding model setup and requirements for local-recall
keywords:
  - embedding
  - fastembed
  - bge-small
  - model
  - local-cache
  - python
  - dependencies
applies_to: global
occurred_at: '2025-12-02T01:15:21.276Z'
content_hash: 5a9b518f743cd4cb
---
# Embedding Model Configuration

Local Recall uses the `fastembed` library with BGE-small-en-v1.5 model for semantic search and memory extraction.

## Requirements

1. **Python**: Required for embedding model (fastembed is Python-based)
2. **Model File**: BGE-small-en-v1.5 (~133MB) is automatically downloaded on first run
3. **Cache Location**: `local_cache/fast-bge-small-en-v1.5/` (auto-created)

## First Run Behavior

- Initial startup takes 30-60 seconds while the model downloads
- Model is cached locally for subsequent runs
- If cache is corrupted, delete `local_cache/` and it will re-download

## Tokenizer Issues

If you see: `Error: Tokenizer file not found at local_cache/fast-bge-small-en-v1.5/tokenizer.json`
- This means the cache download was interrupted or incomplete
- Solution: Delete `rm -rf local_cache/fast-bge-small-en-v1.5*` and let it re-download

## Notes

- The embedding model is essential for semantic search functionality
- Python must be installed and accessible in PATH
- Model downloads happen automatically - no manual configuration needed
